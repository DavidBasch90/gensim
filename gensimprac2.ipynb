{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c820725",
   "metadata": {},
   "source": [
    "<h1>Corpora and Vector Spaces</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98266991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f02ce4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084677",
   "metadata": {},
   "source": [
    "<h2>From strings to vectors</h2>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de323303",
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents represented as strings\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361d87b",
   "metadata": {},
   "source": [
    "First step is to tokenize the documents and remove common words using a (toy) stoplist, as well as remove words that only appear once in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1711ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint #prettyprinter\n",
    "from collections import defaultdict\n",
    "\n",
    "#remove stopwords and tokenize\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "#remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] +=1\n",
    "        \n",
    "        \n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177021fa",
   "metadata": {},
   "source": [
    "To convert the documents to vectors we will use the bag of words representation, in which each document is represented by one vector where each vector element represents a question-answer pair."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd3630",
   "metadata": {},
   "source": [
    "Note: it is advantageous to represent the questions only by their (integer) ids. The mapping between the questions and ids is called a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7398119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:59:09,893 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-10-26 21:59:09,898 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2021-10-26 21:59:09,903 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2021-10-26T21:59:09.901191', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.18362-SP0', 'event': 'created'}\n",
      "2021-10-26 21:59:09,905 : INFO : Dictionary lifecycle event {'fname_or_handle': 'deerwester.dict', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2021-10-26T21:59:09.905334', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.18362-SP0', 'event': 'saving'}\n",
      "2021-10-26 21:59:09,909 : INFO : saved deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('deerwester.dict') #store the dic for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878b14f7",
   "metadata": {},
   "source": [
    "In the above code we are assigning unique numeric ids to each word that appears in our dictionary using the gensim Dictionary class.\n",
    "\n",
    "We can see that there are 12 unique tokens in the corpus, meaning that each document will be represented by 12 numbers -- i.e. a 12 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33f5645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "#lets see the mapping between the words and their ids\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecfcc31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "#lets try running a tokenized document and converting it into a vector\n",
    "\n",
    "new_doc = \"Computer Human Interface Response\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761f2f46",
   "metadata": {},
   "source": [
    "The above vector can be interpreted as a series of question and answers represented by the vectors.\n",
    "\n",
    "The first tuple is a numeric representation of the question: \"how many times does token 0 (computer) appear in the document?\", and the answer \"1\", because the word Computer appears in the new document one time. The remaining tuples are representing similar question/answer pairs: there is one instance of the token 'human'. There is one instance of the token 'interface'. There is one instance of the token 'response'.<br>\n",
    "\n",
    "<b>Note:</b> The above function 'doc2bow' counts the number of occurences of each distinct word, converts the word to its integer id and returns the result as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52e56fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:59:09,955 : INFO : storing corpus in Matrix Market format to deerwester.mm\n",
      "2021-10-26 21:59:09,958 : INFO : saving sparse matrix to deerwester.mm\n",
      "2021-10-26 21:59:09,960 : INFO : PROGRESS: saving document #0\n",
      "2021-10-26 21:59:09,961 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2021-10-26 21:59:09,962 : INFO : saving MmCorpus index to deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#storing the corpus to disk for later use.\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('deerwester.mm', corpus) \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b256f91",
   "metadata": {},
   "source": [
    "<h1> Corpus streaming -- one document at a time. </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690376b0",
   "metadata": {},
   "source": [
    "With toy examples it is acceptable to store the corpus on disk. However, with large number of documents it becomes an issue of storage.\n",
    "In this circumstance it is preferable to store the documents in a file and read them into gensim one document at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbfa86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0379112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 21:59:09,981 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-10-26 21:59:09,984 : INFO : built Dictionary(41 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 67 corpus positions)\n",
      "2021-10-26 21:59:09,986 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(41 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 67 corpus positions)\", 'datetime': '2021-10-26T21:59:09.986535', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.18362-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "dict_STF = corpora.Dictionary(\n",
    "   simple_preprocess(line, deacc =True) for line in open(\"mycorpus.txt\", encoding=\"utf-8\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ba8ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(41 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...)\n"
     ]
    }
   ],
   "source": [
    "print(dict_STF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0479acc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abc': 0, 'applications': 1, 'computer': 2, 'for': 3, 'human': 4, 'interface': 5, 'lab': 6, 'machine': 7, 'of': 8, 'opinion': 9, 'response': 10, 'survey': 11, 'system': 12, 'time': 13, 'user': 14, 'eps': 15, 'management': 16, 'the': 17, 'and': 18, 'engineering': 19, 'testing': 20, 'error': 21, 'measurement': 22, 'perceived': 23, 'relation': 24, 'to': 25, 'binary': 26, 'generation': 27, 'random': 28, 'trees': 29, 'unordered': 30, 'graph': 31, 'in': 32, 'intersection': 33, 'paths': 34, 'iv': 35, 'minors': 36, 'ordering': 37, 'quasi': 38, 'well': 39, 'widths': 40}\n"
     ]
    }
   ],
   "source": [
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c9ae52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11d8e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_memory_friendly = MyCorpus() #corpus is now an object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5859cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x0000022DE26215E0>\n"
     ]
    }
   ],
   "source": [
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a056003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "#this creates a more memory friendly corpus object because at most one vector resides in RAM at a time...\n",
    "for vector in corpus_memory_friendly:\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbc5d469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 22:07:43,807 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-10-26 22:07:43,810 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\n",
      "2021-10-26 22:07:43,811 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\", 'datetime': '2021-10-26T22:07:43.811296', 'gensim': '4.1.2', 'python': '3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.18362-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "#it is also possible to construct the dictionary without loading all texts into memory:\n",
    "\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('mycorpus.txt'))\n",
    "#remove stop words and words that appear only once\n",
    "stop_ids = [\n",
    "    dictionary.token2id[stopword]\n",
    "    for stopword in stoplist\n",
    "    if stopword in dictionary.token2id\n",
    "]\n",
    "\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids) #remove stop words and words that appear only once\n",
    "dictionary.compactify() #remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534f0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
